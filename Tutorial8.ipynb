{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mkkkkkkkkkkkkkkkkkkkk/POSI-MON1/blob/main/Tutorial8.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tutorial 8"
      ],
      "metadata": {
        "id": "j8ER7gHzAgTv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Introduction"
      ],
      "metadata": {
        "id": "CNy2ld8r2uwo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### How does KNN work?\n",
        "\n",
        "The KNN algorithm works by comparing new data with known data (the so-called training data). Its main idea is that objects that are \"close to each other\" (i.e., have similar features) are more alike and should be classified in the same way. The process works as follows:\n",
        "\n",
        "1. **Determine the number of neighbors (K)** - the user selects the number $ K $, which is how many nearest neighbors to consider for classification.\n",
        "2. **Calculate distances** - for a given point (e.g., a new example that we want to classify), the algorithm calculates the distance to all other points in the training set (usually the Euclidean distance is used, but other distances, like Manhattan, can be used).\n",
        "3. **Select the K nearest neighbors** - the algorithm selects $ K $ points from the training dataset that are closest to the point we want to classify.\n",
        "4. **Classification/average** - based on the class (for classification) or value (for regression) of the $ K $ nearest neighbors, the algorithm assigns a label to the new point. In classification, it will usually be the most frequent class among the $ K $ neighbors, and in regression, the average value.\n",
        "\n",
        "<br>\n",
        "\n",
        "#### Example:\n",
        "\n",
        "Let's assume you have a dataset about flowers with two features: petal length and width. You want to classify a new flower. The KNN algorithm will find the $ K $ most similar flowers in the training set (e.g., the 5 nearest ones) and assign a label to the new flower based on the majority (e.g., \"iris-setosa,\" if 3 out of the 5 nearest are setosas).\n",
        "\n",
        "<br>\n",
        "\n",
        "#### Advantages and Disadvantages:\n",
        "\n",
        "##### Advantages:\n",
        "- Easy to understand and implement.\n",
        "- Does not require training a model, works \"on the fly.\"\n",
        "- Can be used for many types of data (e.g., classification, regression).\n",
        "\n",
        "##### Disadvantages:\n",
        "- **Computational performance**: the larger the dataset, the more operations are required.\n",
        "- **Sensitivity to noise and irrelevant features.**\n",
        "- Requires a suitable distance metric (though in some cases it can be difficult to choose).\n"
      ],
      "metadata": {
        "id": "WhehbpCa_FNg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Distance Metrics\n",
        "\n",
        "#### 1. Euclidean Distance\n",
        "\n",
        "This is the most commonly used metric, especially in classic classification and regression tasks when the data is continuous.\n",
        "\n",
        "##### Formula:\n",
        "\n",
        "$$\n",
        "d_E = \\sqrt{(x_1 - y_1)^2 + (x_2 - y_2)^2 + \\dots + (x_n - y_n)^2}\n",
        "$$\n",
        "\n",
        "Where:\n",
        "\n",
        "- $ x_1, x_2, \\dots, x_n $ are the coordinates of point $ x $,\n",
        "- $ y_1, y_2, \\dots, y_n $ are the coordinates of point $ y $,\n",
        "- $ n $ is the number of dimensions (features).\n",
        "\n",
        "##### Example:\n",
        "\n",
        "If we have two points in 2 dimensions: $ x = (3, 4) $ and $ y = (7, 1) $, the Euclidean distance between them is:\n",
        "\n",
        "$$\n",
        "d_E = \\sqrt{(3 - 7)^2 + (4 - 1)^2} = \\sqrt{(-4)^2 + 3^2} = \\sqrt{16 + 9} = \\sqrt{25} = 5\n",
        "$$\n",
        "\n",
        "<br>\n",
        "\n",
        "#### 2. Manhattan Distance\n",
        "\n",
        "This is an alternative measure that sums the differences in coordinates along each axis. It is particularly useful when the data is organized in a grid (e.g., in problems related to movement in cities).\n",
        "\n",
        "##### Formula:\n",
        "\n",
        "$$\n",
        "d_M = |x_1 - y_1| + |x_2 - y_2| + \\dots + |x_n - y_n|\n",
        "$$\n",
        "\n",
        "##### Example:\n",
        "\n",
        "For the points $ x = (3, 4) $ and $ y = (7, 1) $, the Manhattan distance is:\n",
        "\n",
        "$$\n",
        "d_M = |3 - 7| + |4 - 1| = 4 + 3 = 7\n",
        "$$\n",
        "\n",
        "<br>\n",
        "\n",
        "#### 3. Minkowski Distance\n",
        "\n",
        "This is a general form of the two metrics above. The Minkowski distance can take different values depending on the parameter $ p $.\n",
        "\n",
        "##### Formula:\n",
        "\n",
        "$$\n",
        "d_M = \\left( |x_1 - y_1|^p + |x_2 - y_2|^p + \\dots + |x_n - y_n|^p \\right)^{1/p}\n",
        "$$\n",
        "\n",
        "When $ p = 1 $, the Minkowski distance is equal to the Manhattan distance.\n",
        "\n",
        "When $ p = 2 $, it is the Euclidean distance.\n",
        "\n",
        "##### Example:\n",
        "\n",
        "For the points $ x = (3, 4) $ and $ y = (7, 1) $, if $ p = 3 $:\n",
        "\n",
        "$$\n",
        "d_M = \\left( |3 - 7|^3 + |4 - 1|^3 \\right)^{1/3} = \\left( 4^3 + 3^3 \\right)^{1/3} = \\left( 64 + 27 \\right)^{1/3} = 91^{1/3} \\approx 4.5\n",
        "$$\n",
        "\n",
        "<br>\n",
        "\n",
        "#### Summary\n",
        "\n",
        "Depending on the characteristics of the data and the problem, you can choose the appropriate distance metric:\n",
        "\n",
        "- **Euclidean** - most commonly used for general tasks (continuous data).\n",
        "- **Manhattan** - for data that describes \"movement\" in a grid.\n",
        "- **Minkowski** - a general form, allowing experimentation with the parameter $ p $.\n"
      ],
      "metadata": {
        "id": "d_ut8hYYuI6G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise 1\n",
        "For the `load_wine` dataset from the `sklearn.datasets` module, perform a DEA analysis and classify the `target` feature using `KNN`. Test the model's performance for different values of `k-neighbors`. Remember to scale the data.\n",
        "\n",
        "<br>\n",
        "\n",
        "Example of loading data:\n",
        "\n",
        "\n",
        "```\n",
        "from sklearn.datasets import load_wine\n",
        "\n",
        "wine = load_wine()\n",
        "\n",
        "X = wine.data\n",
        "y = wine.target\n",
        "```"
      ],
      "metadata": {
        "id": "LtPGz00R2hsq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "#1 Load and inspect data\n",
        "wine = load_wine()\n",
        "X = wine.data          # features\n",
        "y = wine.target        # classes\n",
        "df = pd.DataFrame(X, columns=wine.feature_names)\n",
        "df[\"target\"] = y\n",
        "print(\"Shape:\", df.shape)\n",
        "print(\"\\nFirst 5 rows:\")\n",
        "print(df.head())\n",
        "print(\"\\nTarget value counts:\")\n",
        "print(df[\"target\"].value_counts())\n",
        "print(\"\\nBasic statistics:\")\n",
        "print(df.describe())\n",
        "#2 Train–test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X,\n",
        "    y,\n",
        "    test_size=0.2,\n",
        "    random_state=42,\n",
        "    stratify=y\n",
        ")\n",
        "#3 Scaling\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "#4 Train KNN for different k and collect results\n",
        "k_values = range(1, 31)\n",
        "results = []\n",
        "for k in k_values:\n",
        "    model = KNeighborsClassifier(n_neighbors=k)\n",
        "    model.fit(X_train_scaled, y_train)\n",
        "\n",
        "    y_train_pred = model.predict(X_train_scaled)\n",
        "    y_test_pred = model.predict(X_test_scaled)\n",
        "\n",
        "    train_acc = accuracy_score(y_train, y_train_pred)\n",
        "    test_acc = accuracy_score(y_test, y_test_pred)\n",
        "\n",
        "    results.append({\n",
        "        \"k\": k,\n",
        "        \"train_acc\": train_acc,\n",
        "        \"test_acc\": test_acc\n",
        "    })\n",
        "\n",
        "    print(f\"k = {k:2d} | train acc = {train_acc:.3f} | test acc = {test_acc:.3f}\")\n",
        "\n",
        "results_df = pd.DataFrame(results)\n",
        "#5 Choose best k (by test accuracy)\n",
        "best_row = results_df.loc[results_df[\"test_acc\"].idxmax()]\n",
        "best_k = int(best_row[\"k\"])\n",
        "print(f\"\\nBest k (by test accuracy) = {best_k}\")\n",
        "print(f\"Train acc = {best_row['train_acc']:.3f}, \"\n",
        "      f\"Test acc = {best_row['test_acc']:.3f}\")\n",
        "#6 Final model & detailed report\n",
        "best_knn = KNeighborsClassifier(n_neighbors=best_k)\n",
        "best_knn.fit(X_train_scaled, y_train)\n",
        "y_test_pred = best_knn.predict(X_test_scaled)\n",
        "print(\"\\nClassification report for best k:\")\n",
        "print(classification_report(y_test, y_test_pred, target_names=wine.target_names))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A8n54Uxpg2YR",
        "outputId": "d2853175-5229-4a80-bd92-35bdb967660f"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape: (178, 14)\n",
            "\n",
            "First 5 rows:\n",
            "   alcohol  malic_acid   ash  alcalinity_of_ash  magnesium  total_phenols  \\\n",
            "0    14.23        1.71  2.43               15.6      127.0           2.80   \n",
            "1    13.20        1.78  2.14               11.2      100.0           2.65   \n",
            "2    13.16        2.36  2.67               18.6      101.0           2.80   \n",
            "3    14.37        1.95  2.50               16.8      113.0           3.85   \n",
            "4    13.24        2.59  2.87               21.0      118.0           2.80   \n",
            "\n",
            "   flavanoids  nonflavanoid_phenols  proanthocyanins  color_intensity   hue  \\\n",
            "0        3.06                  0.28             2.29             5.64  1.04   \n",
            "1        2.76                  0.26             1.28             4.38  1.05   \n",
            "2        3.24                  0.30             2.81             5.68  1.03   \n",
            "3        3.49                  0.24             2.18             7.80  0.86   \n",
            "4        2.69                  0.39             1.82             4.32  1.04   \n",
            "\n",
            "   od280/od315_of_diluted_wines  proline  target  \n",
            "0                          3.92   1065.0       0  \n",
            "1                          3.40   1050.0       0  \n",
            "2                          3.17   1185.0       0  \n",
            "3                          3.45   1480.0       0  \n",
            "4                          2.93    735.0       0  \n",
            "\n",
            "Target value counts:\n",
            "target\n",
            "1    71\n",
            "0    59\n",
            "2    48\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Basic statistics:\n",
            "          alcohol  malic_acid         ash  alcalinity_of_ash   magnesium  \\\n",
            "count  178.000000  178.000000  178.000000         178.000000  178.000000   \n",
            "mean    13.000618    2.336348    2.366517          19.494944   99.741573   \n",
            "std      0.811827    1.117146    0.274344           3.339564   14.282484   \n",
            "min     11.030000    0.740000    1.360000          10.600000   70.000000   \n",
            "25%     12.362500    1.602500    2.210000          17.200000   88.000000   \n",
            "50%     13.050000    1.865000    2.360000          19.500000   98.000000   \n",
            "75%     13.677500    3.082500    2.557500          21.500000  107.000000   \n",
            "max     14.830000    5.800000    3.230000          30.000000  162.000000   \n",
            "\n",
            "       total_phenols  flavanoids  nonflavanoid_phenols  proanthocyanins  \\\n",
            "count     178.000000  178.000000            178.000000       178.000000   \n",
            "mean        2.295112    2.029270              0.361854         1.590899   \n",
            "std         0.625851    0.998859              0.124453         0.572359   \n",
            "min         0.980000    0.340000              0.130000         0.410000   \n",
            "25%         1.742500    1.205000              0.270000         1.250000   \n",
            "50%         2.355000    2.135000              0.340000         1.555000   \n",
            "75%         2.800000    2.875000              0.437500         1.950000   \n",
            "max         3.880000    5.080000              0.660000         3.580000   \n",
            "\n",
            "       color_intensity         hue  od280/od315_of_diluted_wines      proline  \\\n",
            "count       178.000000  178.000000                    178.000000   178.000000   \n",
            "mean          5.058090    0.957449                      2.611685   746.893258   \n",
            "std           2.318286    0.228572                      0.709990   314.907474   \n",
            "min           1.280000    0.480000                      1.270000   278.000000   \n",
            "25%           3.220000    0.782500                      1.937500   500.500000   \n",
            "50%           4.690000    0.965000                      2.780000   673.500000   \n",
            "75%           6.200000    1.120000                      3.170000   985.000000   \n",
            "max          13.000000    1.710000                      4.000000  1680.000000   \n",
            "\n",
            "           target  \n",
            "count  178.000000  \n",
            "mean     0.938202  \n",
            "std      0.775035  \n",
            "min      0.000000  \n",
            "25%      0.000000  \n",
            "50%      1.000000  \n",
            "75%      2.000000  \n",
            "max      2.000000  \n",
            "k =  1 | train acc = 1.000 | test acc = 0.972\n",
            "k =  2 | train acc = 0.972 | test acc = 0.944\n",
            "k =  3 | train acc = 0.972 | test acc = 0.972\n",
            "k =  4 | train acc = 0.958 | test acc = 0.944\n",
            "k =  5 | train acc = 0.979 | test acc = 0.972\n",
            "k =  6 | train acc = 0.965 | test acc = 0.972\n",
            "k =  7 | train acc = 0.965 | test acc = 1.000\n",
            "k =  8 | train acc = 0.965 | test acc = 1.000\n",
            "k =  9 | train acc = 0.965 | test acc = 1.000\n",
            "k = 10 | train acc = 0.972 | test acc = 1.000\n",
            "k = 11 | train acc = 0.965 | test acc = 1.000\n",
            "k = 12 | train acc = 0.965 | test acc = 1.000\n",
            "k = 13 | train acc = 0.965 | test acc = 1.000\n",
            "k = 14 | train acc = 0.972 | test acc = 0.972\n",
            "k = 15 | train acc = 0.979 | test acc = 1.000\n",
            "k = 16 | train acc = 0.965 | test acc = 1.000\n",
            "k = 17 | train acc = 0.972 | test acc = 1.000\n",
            "k = 18 | train acc = 0.965 | test acc = 0.972\n",
            "k = 19 | train acc = 0.972 | test acc = 1.000\n",
            "k = 20 | train acc = 0.972 | test acc = 1.000\n",
            "k = 21 | train acc = 0.972 | test acc = 1.000\n",
            "k = 22 | train acc = 0.979 | test acc = 1.000\n",
            "k = 23 | train acc = 0.979 | test acc = 1.000\n",
            "k = 24 | train acc = 0.979 | test acc = 1.000\n",
            "k = 25 | train acc = 0.979 | test acc = 1.000\n",
            "k = 26 | train acc = 0.979 | test acc = 1.000\n",
            "k = 27 | train acc = 0.972 | test acc = 1.000\n",
            "k = 28 | train acc = 0.972 | test acc = 0.972\n",
            "k = 29 | train acc = 0.972 | test acc = 0.972\n",
            "k = 30 | train acc = 0.972 | test acc = 0.972\n",
            "\n",
            "Best k (by test accuracy) = 7\n",
            "Train acc = 0.965, Test acc = 1.000\n",
            "\n",
            "Classification report for best k:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     class_0       1.00      1.00      1.00        12\n",
            "     class_1       1.00      1.00      1.00        14\n",
            "     class_2       1.00      1.00      1.00        10\n",
            "\n",
            "    accuracy                           1.00        36\n",
            "   macro avg       1.00      1.00      1.00        36\n",
            "weighted avg       1.00      1.00      1.00        36\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise 2\n",
        "For the `fetch_california_housing` dataset from the `sklearn.datasets` module, perform a DEA analysis and regression using `KNN`. Test the model's performance for different values of `k-neighbors`. Remember to scale the data.\n",
        "\n",
        "<br>\n",
        "\n",
        "Example of loading data:\n",
        "\n",
        "```\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "\n",
        "data = fetch_california_housing()\n",
        "X = data.data\n",
        "y = data.target\n",
        "```"
      ],
      "metadata": {
        "id": "ga00M6JV2Vqh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "# 1 Load data\n",
        "data = fetch_california_housing()\n",
        "X = data.data\n",
        "y = data.target  # median house value\n",
        "\n",
        "df = pd.DataFrame(X, columns=data.feature_names)\n",
        "df[\"target\"] = y\n",
        "\n",
        "print(\"Shape:\", df.shape)\n",
        "print(\"\\nFirst 5 rows:\")\n",
        "print(df.head())\n",
        "\n",
        "print(\"\\nBasic statistics:\")\n",
        "print(df.describe())\n",
        "\n",
        "print(\"\\nCorrelation with target:\")\n",
        "print(df.corr(numeric_only=True)[\"target\"].sort_values(ascending=False))\n",
        "\n",
        "# 2 Train–test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# 3 Scaling\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# 4 Try different k values\n",
        "k_values = range(1, 31)\n",
        "results = []\n",
        "\n",
        "for k in k_values:\n",
        "    knn_reg = KNeighborsRegressor(n_neighbors=k)\n",
        "    knn_reg.fit(X_train_scaled, y_train)\n",
        "    y_train_pred = knn_reg.predict(X_train_scaled)\n",
        "    y_test_pred = knn_reg.predict(X_test_scaled)\n",
        "    train_mse = mean_squared_error(y_train, y_train_pred)\n",
        "    test_mse = mean_squared_error(y_test, y_test_pred)\n",
        "    train_rmse = np.sqrt(train_mse)\n",
        "    test_rmse = np.sqrt(test_mse)\n",
        "\n",
        "    train_r2 = r2_score(y_train, y_train_pred)\n",
        "    test_r2 = r2_score(y_test, y_test_pred)\n",
        "\n",
        "    results.append((k, train_rmse, test_rmse, train_r2, test_r2))\n",
        "    print(\n",
        "        f\"k = {k:2d} | \"\n",
        "        f\"train RMSE = {train_rmse:.3f}, test RMSE = {test_rmse:.3f}, \"\n",
        "        f\"train R2 = {train_r2:.3f}, test R2 = {test_r2:.3f}\"\n",
        "    )\n",
        "\n",
        "# 5 Choose the best k (minimal test RMSE)\n",
        "test_rmses = [r[2] for r in results]\n",
        "best_k_index = int(np.argmin(test_rmses))\n",
        "best_k = k_values[best_k_index]\n",
        "best_k_results = results[best_k_index]\n",
        "\n",
        "print(f\"\\nBest k (by test RMSE) = {best_k}\")\n",
        "print(\n",
        "    f\"Best k metrics -> \"\n",
        "    f\"train RMSE = {best_k_results[1]:.3f}, test RMSE = {best_k_results[2]:.3f}, \"\n",
        "    f\"train R2 = {best_k_results[3]:.3f}, test R2 = {best_k_results[4]:.3f}\"\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kc2zzJfhknAR",
        "outputId": "a1f9091c-a61d-46a6-c1e6-150e65b41709"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape: (20640, 9)\n",
            "\n",
            "First 5 rows:\n",
            "   MedInc  HouseAge  AveRooms  AveBedrms  Population  AveOccup  Latitude  \\\n",
            "0  8.3252      41.0  6.984127   1.023810       322.0  2.555556     37.88   \n",
            "1  8.3014      21.0  6.238137   0.971880      2401.0  2.109842     37.86   \n",
            "2  7.2574      52.0  8.288136   1.073446       496.0  2.802260     37.85   \n",
            "3  5.6431      52.0  5.817352   1.073059       558.0  2.547945     37.85   \n",
            "4  3.8462      52.0  6.281853   1.081081       565.0  2.181467     37.85   \n",
            "\n",
            "   Longitude  target  \n",
            "0    -122.23   4.526  \n",
            "1    -122.22   3.585  \n",
            "2    -122.24   3.521  \n",
            "3    -122.25   3.413  \n",
            "4    -122.25   3.422  \n",
            "\n",
            "Basic statistics:\n",
            "             MedInc      HouseAge      AveRooms     AveBedrms    Population  \\\n",
            "count  20640.000000  20640.000000  20640.000000  20640.000000  20640.000000   \n",
            "mean       3.870671     28.639486      5.429000      1.096675   1425.476744   \n",
            "std        1.899822     12.585558      2.474173      0.473911   1132.462122   \n",
            "min        0.499900      1.000000      0.846154      0.333333      3.000000   \n",
            "25%        2.563400     18.000000      4.440716      1.006079    787.000000   \n",
            "50%        3.534800     29.000000      5.229129      1.048780   1166.000000   \n",
            "75%        4.743250     37.000000      6.052381      1.099526   1725.000000   \n",
            "max       15.000100     52.000000    141.909091     34.066667  35682.000000   \n",
            "\n",
            "           AveOccup      Latitude     Longitude        target  \n",
            "count  20640.000000  20640.000000  20640.000000  20640.000000  \n",
            "mean       3.070655     35.631861   -119.569704      2.068558  \n",
            "std       10.386050      2.135952      2.003532      1.153956  \n",
            "min        0.692308     32.540000   -124.350000      0.149990  \n",
            "25%        2.429741     33.930000   -121.800000      1.196000  \n",
            "50%        2.818116     34.260000   -118.490000      1.797000  \n",
            "75%        3.282261     37.710000   -118.010000      2.647250  \n",
            "max     1243.333333     41.950000   -114.310000      5.000010  \n",
            "\n",
            "Correlation with target:\n",
            "target        1.000000\n",
            "MedInc        0.688075\n",
            "AveRooms      0.151948\n",
            "HouseAge      0.105623\n",
            "AveOccup     -0.023737\n",
            "Population   -0.024650\n",
            "Longitude    -0.045967\n",
            "AveBedrms    -0.046701\n",
            "Latitude     -0.144160\n",
            "Name: target, dtype: float64\n",
            "k =  1 | train RMSE = 0.000, test RMSE = 0.818, train R2 = 1.000, test R2 = 0.489\n",
            "k =  2 | train RMSE = 0.399, test RMSE = 0.723, train R2 = 0.881, test R2 = 0.601\n",
            "k =  3 | train RMSE = 0.468, test RMSE = 0.683, train R2 = 0.836, test R2 = 0.644\n",
            "k =  4 | train RMSE = 0.502, test RMSE = 0.669, train R2 = 0.812, test R2 = 0.659\n",
            "k =  5 | train RMSE = 0.521, test RMSE = 0.658, train R2 = 0.797, test R2 = 0.670\n",
            "k =  6 | train RMSE = 0.537, test RMSE = 0.655, train R2 = 0.784, test R2 = 0.673\n",
            "k =  7 | train RMSE = 0.549, test RMSE = 0.654, train R2 = 0.774, test R2 = 0.673\n",
            "k =  8 | train RMSE = 0.558, test RMSE = 0.652, train R2 = 0.767, test R2 = 0.676\n",
            "k =  9 | train RMSE = 0.565, test RMSE = 0.652, train R2 = 0.761, test R2 = 0.676\n",
            "k = 10 | train RMSE = 0.572, test RMSE = 0.649, train R2 = 0.755, test R2 = 0.678\n",
            "k = 11 | train RMSE = 0.578, test RMSE = 0.647, train R2 = 0.750, test R2 = 0.681\n",
            "k = 12 | train RMSE = 0.582, test RMSE = 0.647, train R2 = 0.746, test R2 = 0.681\n",
            "k = 13 | train RMSE = 0.587, test RMSE = 0.645, train R2 = 0.743, test R2 = 0.682\n",
            "k = 14 | train RMSE = 0.592, test RMSE = 0.646, train R2 = 0.738, test R2 = 0.681\n",
            "k = 15 | train RMSE = 0.595, test RMSE = 0.647, train R2 = 0.735, test R2 = 0.680\n",
            "k = 16 | train RMSE = 0.598, test RMSE = 0.649, train R2 = 0.733, test R2 = 0.679\n",
            "k = 17 | train RMSE = 0.600, test RMSE = 0.649, train R2 = 0.730, test R2 = 0.679\n",
            "k = 18 | train RMSE = 0.603, test RMSE = 0.649, train R2 = 0.728, test R2 = 0.678\n",
            "k = 19 | train RMSE = 0.605, test RMSE = 0.649, train R2 = 0.726, test R2 = 0.678\n",
            "k = 20 | train RMSE = 0.608, test RMSE = 0.651, train R2 = 0.724, test R2 = 0.677\n",
            "k = 21 | train RMSE = 0.610, test RMSE = 0.651, train R2 = 0.722, test R2 = 0.676\n",
            "k = 22 | train RMSE = 0.612, test RMSE = 0.652, train R2 = 0.720, test R2 = 0.676\n",
            "k = 23 | train RMSE = 0.614, test RMSE = 0.652, train R2 = 0.718, test R2 = 0.675\n",
            "k = 24 | train RMSE = 0.616, test RMSE = 0.653, train R2 = 0.716, test R2 = 0.675\n",
            "k = 25 | train RMSE = 0.617, test RMSE = 0.653, train R2 = 0.715, test R2 = 0.675\n",
            "k = 26 | train RMSE = 0.619, test RMSE = 0.654, train R2 = 0.713, test R2 = 0.674\n",
            "k = 27 | train RMSE = 0.620, test RMSE = 0.654, train R2 = 0.712, test R2 = 0.673\n",
            "k = 28 | train RMSE = 0.622, test RMSE = 0.656, train R2 = 0.711, test R2 = 0.672\n",
            "k = 29 | train RMSE = 0.624, test RMSE = 0.656, train R2 = 0.709, test R2 = 0.672\n",
            "k = 30 | train RMSE = 0.625, test RMSE = 0.657, train R2 = 0.708, test R2 = 0.671\n",
            "\n",
            "Best k (by test RMSE) = 13\n",
            "Best k metrics -> train RMSE = 0.587, test RMSE = 0.645, train R2 = 0.743, test R2 = 0.682\n"
          ]
        }
      ]
    }
  ]
}